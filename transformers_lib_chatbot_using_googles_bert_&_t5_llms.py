# -*- coding: utf-8 -*-
"""Transformers Lib - Chatbot Using Googles BERT & T5 LLMs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z4cNa5itpBIeCOsPNvhXqR7MY-hABgpd

# Transformers Library (Hugging Face) - Testing Googles BERT & T5
"""

import os
from IPython.display import Image
PATH = "C:\\Users\\pitsi\\Desktop\\2024 Youtube\\1.OpenAI_Chatbot\\"
Image("/content/AI Pic 4.PNG",width=900, height=700)

"""## Agenda

1. Why choose Transformers
2. Libraries to install
3. How to etract the text from PDF
4. How to summarize the text
5. How to question the text
6. How to deploy - Streamlit App
7. Compare with the API LLMs

#### Transformers Library (Hugging Face Transformers):
The Transformers library allows you to download, fine-tune, and use models locally on your own machine or infrastructure. It gives you full control over how you work with the models.

When using LLM models, you generally have two main options: using the Transformers library (e.g., Hugging Face's transformers) or calling models via APIs (such as OpenAI, Google, or Hugging Face APIs). Both approaches have their pros and cons depending on your project
"""

Image("/content/transf Vs API 1.png",width=900, height=700)

Image("transf Vs API 2.png",width=900, height=700)

"""## 1. Libraries to install"""

# pip install openai PyPDF2 pdfplumber

# pip install streamlit

# pip install --upgrade plotly

# pip install pymupdf

# pip install transformers

# pip install torch

# pip install tensorflow

# pip install sentencepiece

# pip install tf-keras

# Install necessary libraries
pip install streamlit
pip install --upgrade plotly
pip install pymupdf
pip install transformers
pip install torch
pip install tensorflow
pip install sentencepiece
pip install tf-keras
pip install pdfplumber
pip install PyPDF2

python transformers_lib_chatbot_using_googles_bert_&_t5_llms.py



# Importing our libraries

# import openai
import pdfplumber
import os

import fitz  # PyMuPDF for PDF extraction
from transformers import T5Tokenizer, T5ForConditionalGeneration, BertTokenizer, BertForQuestionAnswering, pipeline

"""## 2. Extracting Text from PDF"""

def extract_text_from_pdf(pdf_path):
    """Extract text from a PDF file."""
    doc = fitz.open(pdf_path)
    text = ''
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        text += page.get_text()
    return text

text = extract_text_from_pdf('/content/Bitwise-The-Year-Ahead-10-Crypto-Predictions-for-2024.pdf')
text

"""## 3. Summarizes the text using Google's T5"""

# Initialize T5 model and tokenizer for summarization
t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')
t5_model = T5ForConditionalGeneration.from_pretrained('t5-small')

def summarize_text(text):
    """Summarize the given text using T5."""
    inputs = t5_tokenizer.encode("summarize: " + text, return_tensors="pt", max_length=1024, truncation=True)
    outputs = t5_model.generate(inputs, max_length=150, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)
    summary = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)
    return summary

# doc: https://huggingface.co/docs/transformers/en/model_doc/t5

summarize_text(text)

"""## 4. Asks a question about the text using Google's Bert"""

# Initialize BERT model and tokenizer for question answering
qa_pipeline = pipeline('question-answering', model='bert-large-uncased', tokenizer='bert-large-uncased')


def answer_question(question, context):
    """Answer a question based on the context using BERT."""
    result = qa_pipeline(question=question, context=context)
    return result['answer']

## doc: https://cloud.google.com/ai-platform/training/docs/algorithms/reference/bert

# Example questions
questions = [
    "What is the main topic of the document?",
    "Who are the key people mentioned?",
]

for question in questions:
    answer = answer_question(question, text)
    print(f"Question: {question}")
    print(f"Answer: {answer}")

"""## Creating the main() function and putting it all together"""

import PyPDF2
from transformers import T5Tokenizer, T5ForConditionalGeneration, pipeline

# Initialize T5 model and tokenizer for summarization
t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')
t5_model = T5ForConditionalGeneration.from_pretrained('t5-small')

# Initialize BERT model and tokenizer for question answering
qa_pipeline = pipeline('question-answering', model='bert-large-uncased', tokenizer='bert-large-uncased')

def extract_text_from_pdf(pdf_path):
    """Extract text from a PDF file."""
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

def summarize_text(text):
    """Summarize the given text using T5."""
    inputs = t5_tokenizer.encode("summarize: " + text, return_tensors="pt", max_length=1024, truncation=True)
    outputs = t5_model.generate(inputs, max_length=150, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)
    summary = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)
    return summary

def answer_question(question, context):
    """Answer a question based on the context using BERT."""
    result = qa_pipeline(question=question, context=context)
    return result['answer']

# Example usage
pdf_path = 'Bitwise-The-Year-Ahead-10-Crypto-Predictions-for-2024.pdf'
text = extract_text_from_pdf(pdf_path)
summary = summarize_text(text)

print("Summary:")
print(summary)

# Example questions
questions = [
    "What is the main topic of the document?",
    "Who are the key people mentioned?",
]

for question in questions:
    answer = answer_question(question, text)
    print(f"Question: {question}")
    print(f"Answer: {answer}")

"""## Deploying this in a Streamlit app"""

import streamlit as st
import PyPDF2
from transformers import T5Tokenizer, T5ForConditionalGeneration, pipeline

# Initialize models and tokenizers
t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')
t5_model = T5ForConditionalGeneration.from_pretrained('t5-small')
qa_pipeline = pipeline('question-answering', model='bert-large-uncased', tokenizer='bert-large-uncased')

def extract_text_from_pdf(pdf_file):
    """Extract text from the uploaded PDF file."""
    text = ""
    with pdf_file as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

def summarize_text(text):
    """Summarize the given text using T5."""
    inputs = t5_tokenizer.encode("summarize: " + text, return_tensors="pt", max_length=1024, truncation=True)
    outputs = t5_model.generate(inputs, max_length=150, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)
    summary = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)
    return summary

def answer_question(question, context):
    """Answer a question based on the context using BERT."""
    result = qa_pipeline(question=question, context=context)
    return result['answer']

# Streamlit app
st.title("PDF Summarizer and Question Answering")

uploaded_file = st.file_uploader("Choose a PDF file", type="pdf")

if uploaded_file is not None:
    # Extract text from PDF
    text = extract_text_from_pdf(uploaded_file)

    # Limit the text displayed to 500 characters
    display_text = text[:500] + ('...' if len(text) > 500 else '')

    # Display the extracted text
    st.subheader("Extracted Text")
    st.text_area("Text from PDF", display_text, height=300)

    # Summarize text
    if st.button("Summarize"):
        summary = summarize_text(text)
        st.subheader("Summary")
        st.write(summary)

    # Answer questions
    question = st.text_input("Enter your question about the PDF:")
    if question:
        answer = answer_question(question, text)
        st.subheader("Answer")
        st.write(answer)

